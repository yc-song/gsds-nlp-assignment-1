{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Q3: Sentence Representation\n",
    "\n",
    "Based on word representation which we learned from Question 1 and 2, we will represent sentence by averag-ing vectors of words consisting of sentences. Skeleton code is provided on this file. Every methods and functions are presented for you. What you are supposed to do is just run those codes and write down your answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# ----------------\n",
    "\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "from platform import python_version\n",
    "assert int(python_version().split(\".\")[1]) >= 5, \"Please upgrade your Python version following the instructions in \\\n",
    "    the README.txt file found in the same directory as this notebook. Your Python version is \" + python_version()\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import nltk\n",
    "nltk.download('reuters') #to specify download location, optionally add the argument: download_dir='/specify/desired/path/'\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will be using the Reuters (business and financial news) corpus. If you haven't run the import cell at the top of this page, please run it now (click it and press SHIFT-RETURN). The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see https://www.nltk.org/book/ch02.html. You do **not** have to perform any other kind of pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids()\n",
    "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what these documents are likeâ€¦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reuters_corpus = read_corpus()\n",
    "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Representation of Sentences\n",
    "\n",
    "As discussed in class, more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe . Here, we shall represent the sentence by averaging word embeddings produced by GloVe. If you want to know further details of GloVe, try reading [GloVe's original paper](https://nlp.stanford.edu/pubs/glove.pdf).\n",
    "\n",
    "Then run the following cells to load the GloVe vectors into memory. **Note**: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Run Cell to Load Word Vectors\n",
    "# Note: This will take a couple minutes\n",
    "# -----------------------------------\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "(1) If you are receiving a \"reset by peer\" error, rerun the cell to restart the download. \n",
    "\n",
    "(2) If you are receiving out of memory issues on your local machine, try closing other applications to free more memory on your device. You may want to try restarting your machine so that you can free up extra memory. Then immediately run the jupyter notebook and see if you can load the word vectors properly. If you still have problems with loading the embeddings onto your local machine after this, please go to office hours or contact course TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (a): Tokenization\n",
    "Tokenization splits a sentence (string) into tokens, rough equivalent to words and punctuation. For example, to process the sentence 'I love New York', the given sentence need to be tokenized to ['I', 'love', 'New', 'York']. Many NLP libraries and packages support tokenization, because it is one of the most fundamental steps in NLP pipeline. However, there is no standard solution that every NLP practitioners agrees upon. Let's compare how different NLP packages tokenize sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=\"The BBC's correspondent in Athens, Malcolm Brabant, said that in the past few weeks more details had emerged of the alleged mistreatment by Greek-speaking agents.\"\n",
    "sentence2=\"A new chapter has been written into Australia's rich sporting history after the Socceroos qualified for the World Cup finals following their 4-2 win over Uruguay on penalties at the Olympic Stadium in Sydney.\"\n",
    "\n",
    "print(\"tokenization of sentence 1\", word_tokenize(sentence1))\n",
    "print(\"tokenization of sentence 1\", WordPunctTokenizer().tokenize(sentence1))\n",
    "print(\"tokenization of sentence 2\", word_tokenize(sentence2))\n",
    "print(\"tokenization of sentence 2\", WordPunctTokenizer().tokenize(sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (b): Stopword\n",
    "Stop words are the words in a stop list which are filtered out (i.e. stopped) before or after processing of natural language data (text). Let's check out the english stopwords list of NLTK as running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "print('# of stop word list :', len(stop_words_list))\n",
    "print('The whole stop word list',stop_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code and skim the list. State ***TWO*** reasons why those stopwords are filtered out during the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (c)\n",
    "\n",
    "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
    "\n",
    "Find three sentences $(s_1,s_2,s_3)$ where $s_1$ and $s_2$ are sentences which have similar meanin and $s_1$ and $s_3$ are antonyms, but Cosine Distance $(s_1,s_3) <$ Cosine Distance $(s_1,s_2)$. \n",
    "\n",
    "As an example, $s_1$=\"I like everything of this movie. The only thing I do not like is the cast.\" is closer to $s_3$=\"I do not like everything of this movie. The only thing I like is the cast.\" than to $s_2$=\"I love all about this movie.\" in the vector space. Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
    "\n",
    "You should use the the `counter_intuitive_sentences` function which returns true when the condition above is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def counter_intuitive_sentences(s1:str,s2:str,s3:str)->bool:\n",
    "    s1_embedding=sentence_embedding(s1)\n",
    "    s2_embedding=sentence_embedding(s2)\n",
    "    s3_embedding=sentence_embedding(s3)\n",
    "    if (cos_distance(s1_embedding,s3_embedding)<cos_distance(s1_embedding,s2_embedding)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def cos_distance(a:np.ndarray, b:np.ndarray)->float:\n",
    "    distance=1-math.fsum(a*b)/math.sqrt(math.fsum(a*a)*math.fsum(b*b))\n",
    "    return distance\n",
    "\n",
    "def sentence_embedding(s: str)->np.ndarray:\n",
    "    s=s.lower()\n",
    "    s=WordPunctTokenizer().tokenize(s)\n",
    "    s_embedding=np.zeros([200,], dtype=np.double)\n",
    "    stop_words=set(stopwords.words('english')) \n",
    "    count=0\n",
    "    for word in s:\n",
    "        if word not in stop_words:\n",
    "            s_embedding+=wv_from_bin.get_vector(word)\n",
    "            count+=1\n",
    "    \n",
    "    s_embedding=s_embedding/count\n",
    "    return s_embedding\n",
    "\n",
    "s1=\"I like everything of this movie. The only thing I do not like is the cast.\"\n",
    "s2=\"I love all about this movie.\"\n",
    "s3=\"I do not like everything of this movie. The only thing I like is the cast.\" \n",
    "    \n",
    "counter_intuitive_sentences(s1,s2,s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR EXAMPLE HERE ####\n",
    "s1=\"\"\n",
    "s2=\"\"\n",
    "s3=\"\"\n",
    "#### BELOW SHOULD RETURN TRUE\n",
    "print(counter_intuitive_sentences(s1,s2,s3))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cae7ac32d8cff5c1e4cf23dee71608b7fd2b7ba28595b4ee9b28a0ddab2d1bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
